The network _d2-3200-v1-1-1_ uses 3200 bit rate Codec 2. 

The network has eventually arrived at 5 LSTMs of 160 timesteps. One benefit of the 3200 bit rate codec
is that there is only one 'voiced' bit per frame, reducing the number of properties to be learned to 13. 

From a learning perspective, having a single voicing bit also should be easier, since every 
property has exactly one frame scope. With the 1300 bit rate codec there are 4 voiced bits per frame,
requiring learning of timed relationships within a single frame.

The best loss achieved is at around frame 1910, measuing ~0.41

Just after iteration 1910 the loss starts to jump to around 5. Looking at the parameter plots, it seems
that the generator has got stuck just producing a suboptimal mean. Stepping back to just before this and the 
audio results are reasonable.

